{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, copy, ntpath\n",
    "from keras import models\n",
    "from keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory definitions\n",
    "# this code has changed since last tested/used\n",
    "\n",
    "original_dataset_dir = r'C:\\Users\\aln15\\Documents\\SMUHomeWork\\Capstone\\asl_alphabet_train\\asl_alphabet_train'\n",
    "base_dir =  r'C:\\Users\\aln15\\Documents\\SMUHomeWork\\Capstone\\handsigns'\n",
    "#base_dir = 'C:/Users/aln15/Documents/SMUHomeWork/Capstone/handsigns'\n",
    "\n",
    "#os.mkdir(base_dir)\n",
    "\n",
    "train_dir = os.path.join(base_dir,'train_asl')\n",
    "\n",
    "#os.mkdir(train_dir)\n",
    "\n",
    "validation_dir = os.path.join(base_dir,'validation_asl')\n",
    "#os.mkdir(validation_dir)\n",
    "\n",
    "\n",
    "test_dir = os.path.join(base_dir,'test_asl')\n",
    "#os.mkdir(test_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # copy datafiles into directory structure for modelling\n",
    "   #untested since last change\n",
    "\n",
    "asl_folders = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z','del','space','nothing']\n",
    "\n",
    "\n",
    "for signFolder in range (len(asl_folders)):\n",
    "   sign_dir = os.path.join(original_dataset_dir, asl_folders[signFolder])\n",
    "   letter_folder = os.path.join(asl_folders[signFolder] + str(\"\\\\\"))\n",
    "\n",
    "   test_out_dir = ntpath.dirname(os.path.join(test_dir, letter_folder))\n",
    "   #os.mkdir(test_out_dir)\n",
    "\n",
    "   val_out_dir = ntpath.dirname(os.path.join(var_dir, letter_folder))\n",
    "   #os.mkdir(val_out_dir)\n",
    "\n",
    "   train_out_dir = ntpath.dirname(os.path.join(train_dir, letter_folder))\n",
    "    #os.mkdir(train_out_dir)\n",
    "\n",
    "for fileCounter in range (3000,3001):\n",
    "        fname = asl_folders[signFolder]+str(fileCounter) + '.jpg'\n",
    "        src = os.path.join(sign_dir,fname)\n",
    "        \n",
    "        if fileCounter <=1200:\n",
    "            dat = ntpath.dirname(os.path.join(train_out_dir))\n",
    "        if fileCounter > 1200 and fileCounter <=1950: \n",
    "            dat = ntpath.dirname(os.path.join(validation_out_dir))\n",
    "        if fileCounter > 1950 and fileCounter <=3000:\n",
    "            dat = ntpath.dirname(os.path.join(test_out_dir))\n",
    "                     \n",
    "        shutil.copy(src, out_dir)\n",
    "        #print(\"Source: \", src, \"Destination: \", out_dir)\n",
    "\n",
    "    # 29 signs                      \n",
    "    # number of training samples/sign = 1749\n",
    "    # number of validation samples/sign = 501\n",
    "    # number of test samples/sign = 750\n",
    "    # of training samples total = 50,721\n",
    "    # of validation samples total = 14,429\n",
    "    # of test samples total = 21,750\n",
    "    \n",
    "    # balanced multi-class problem\n",
    "    #classification accuracy is the appropriate measure of success\n",
    "    \n",
    "    #network architecture notes\n",
    "    # final layer will have density size of 29 (number of signs) and activation of softmax \n",
    "    #Convnet = stack of alternated Conv2D (with relu activation and MaxPooling2D layers\n",
    "    #one layer of Conv2D + MaxPooling2D  (increases the capacity of the netwwork and reduces the size of the feature maps prior     #to the Flatten layer\n",
    "    #input size of 28 x 28\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data generation setip\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "#image size (original = 28)\n",
    "tarsize = 28\n",
    "batchsize = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for tensorboard display (doesn't work yet)\n",
    "my_log_dir = ntpath.dirname(r'C:\\Users\\aln15\\Documents\\SMUHomeWork\\Capstone\\hsmodellogs')\n",
    "#os.mkdir(my_log_dir)\n",
    "tensorboard = keras.callbacks.TensorBoard(\n",
    "    log_dir=my_log_dir,\n",
    "    histogram_freq=1,\n",
    "    write_images=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#augnmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "           rotation_range = 0,\n",
    "           width_shift_range = .2,\n",
    "           height_shift_range = .2,\n",
    "           shear_range = .2,\n",
    "           zoom_range = .2,\n",
    "           horizontal_flip = False,\n",
    "           fill_mode = 'nearest'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "#---------end augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset definition without augmentation\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "# end without augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50721 images belonging to 29 classes.\n",
      "Found 14529 images belonging to 29 classes.\n",
      "Found 21750 images belonging to 29 classes.\n"
     ]
    }
   ],
   "source": [
    "#create generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "           train_dir,\n",
    "           target_size = (tarsize,tarsize ),\n",
    "           batch_size=batchsize,\n",
    "           class_mode = 'categorical'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "         validation_dir,\n",
    "         target_size = (tarsize,tarsize),\n",
    "         batch_size = batchsize,\n",
    "         class_mode='categorical'\n",
    ")\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "         test_dir,\n",
    "         target_size = (tarsize,tarsize),\n",
    "         batch_size = batchsize,\n",
    "         class_mode='categorical'\n",
    ")\n",
    "\n",
    "\n",
    "#------end of dataset definitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               73856     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 29)                3741      \n",
      "=================================================================\n",
      "Total params: 133,917\n",
      "Trainable params: 133,917\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 92s 923ms/step - loss: 3.3375 - acc: 0.0420 - val_loss: 3.2839 - val_acc: 0.0917\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 38s 377ms/step - loss: 3.1561 - acc: 0.1080 - val_loss: 3.3987 - val_acc: 0.0402\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 37s 366ms/step - loss: 2.8381 - acc: 0.1935 - val_loss: 2.8553 - val_acc: 0.1880\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 36s 363ms/step - loss: 2.4616 - acc: 0.2765 - val_loss: 2.8510 - val_acc: 0.1735\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 38s 377ms/step - loss: 2.2558 - acc: 0.3215 - val_loss: 2.4830 - val_acc: 0.2694\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 36s 359ms/step - loss: 1.9652 - acc: 0.3820 - val_loss: 2.1680 - val_acc: 0.3495\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 36s 364ms/step - loss: 1.7862 - acc: 0.4479 - val_loss: 2.2122 - val_acc: 0.3139\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 38s 377ms/step - loss: 1.6180 - acc: 0.4835 - val_loss: 2.1437 - val_acc: 0.3154\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 37s 372ms/step - loss: 1.5509 - acc: 0.5120 - val_loss: 1.8998 - val_acc: 0.4198\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 36s 361ms/step - loss: 1.4482 - acc: 0.5295 - val_loss: 1.7746 - val_acc: 0.4134\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 37s 365ms/step - loss: 1.3525 - acc: 0.5555 - val_loss: 1.7427 - val_acc: 0.4316\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 37s 374ms/step - loss: 1.2224 - acc: 0.6025 - val_loss: 1.7000 - val_acc: 0.4127\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 36s 364ms/step - loss: 1.2395 - acc: 0.5885 - val_loss: 1.6502 - val_acc: 0.4587\n",
      "Epoch 14/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 1.1366 - acc: 0.6369"
     ]
    }
   ],
   "source": [
    "#modelling...setup, definition, compile, run and test\n",
    "\n",
    "callbacks_list = [\n",
    " #        tensorboard,\n",
    "         keras.callbacks.EarlyStopping(monitor='acc', patience=2),\n",
    "         keras.callbacks.ModelCheckpoint(filepath = 'handsigns.h5', monitor = 'val_loss', save_best_only = True),\n",
    "         keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor = 0.1, patience=10)\n",
    "]\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(tarsize, tarsize, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "#Add a classfier on top of the convnet\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(.5))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "#model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(29, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "#compile the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history= model.fit_generator (\n",
    "           train_generator,\n",
    "           steps_per_epoch = 100,\n",
    "           epochs = 100,\n",
    "           callbacks = callbacks_list,\n",
    "           validation_data=validation_generator,\n",
    "           verbose=1,\n",
    "           #shuffle = FALSE,\n",
    "           validation_steps = 50)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_loss, test_acc = model.evaluate_generator(test_generator,steps=50)\n",
    "print('test_acc: ',test_acc)\n",
    "\n",
    "#save the model\n",
    "model.save('handsigns_try2aug.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization of model performance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values =  history_dict['val_loss']\n",
    "acc_values = history_dict['acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_values, 'bo', label = 'Training Loss')\n",
    "plt.plot(epochs,val_loss_values, 'b', label = 'Validation Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.xlabel='Epochs'\n",
    "plt.ylabel='Loss'\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
    "plt.plot(epochs,val_acc_values, 'b', label='Validation accuracy')\n",
    "plt.xlabel = 'Epochs'\n",
    "plt.ylabel = 'Loss'\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
