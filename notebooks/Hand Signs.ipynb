{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, shutil, copy, ntpath, datetime\n",
    "import keras\n",
    "from keras import models, layers, callbacks\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import boto3\n",
    "import glob\n",
    "import ntpath\n",
    "import re\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Settings\n",
    "IMAGE_SHAPE=(150, 150, 3)\n",
    "\n",
    "#LIMIT_BATCHES = 2\n",
    "LIMIT_BATCHES = None\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 20\n",
    "NUM_CLASSES = 29\n",
    "\n",
    "# PRETRAIN_MODEL = 'conv_base_local'\n",
    "# PRETRAIN_MODEL = 'conv_base_vgg16'\n",
    "PRETRAIN_MODEL = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FilePath Information\n",
    "DATA_DIR = '../data'\n",
    "ASL_ALPHABET_DATASET = os.path.join(DATA_DIR, 'original/asl_alphabet')\n",
    "\n",
    "\n",
    "SAVED_MODEL_DIR = os.path.join(DATA_DIR, 'models/asl_alphabet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data generation setip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset definition without augmentation\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.1)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "# end without augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 78289 images belonging to 29 classes.\n",
      "Found 8698 images belonging to 29 classes.\n"
     ]
    }
   ],
   "source": [
    "#create generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "           ASL_ALPHABET_DATASET,\n",
    "           target_size = (IMAGE_SHAPE[0], IMAGE_SHAPE[0]),\n",
    "           batch_size=BATCH_SIZE,\n",
    "           class_mode = 'categorical',\n",
    "           subset='training'\n",
    ")\n",
    "\n",
    "#create generators\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "           ASL_ALPHABET_DATASET,\n",
    "           target_size = (IMAGE_SHAPE[0], IMAGE_SHAPE[0]),\n",
    "           batch_size=BATCH_SIZE,\n",
    "           class_mode = 'categorical',\n",
    "           subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 15, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 5, 5, 256)         295168    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               819328    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 29)                3741      \n",
      "=================================================================\n",
      "Total params: 1,215,677\n",
      "Trainable params: 1,215,677\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "callbacks_list = [\n",
    "         callbacks.EarlyStopping(monitor='acc', patience=2),\n",
    "         callbacks.ModelCheckpoint(filepath = '../data/models/handsigns/handsigns.h5', monitor = 'val_loss', save_best_only = True),\n",
    "         callbacks.ReduceLROnPlateau(monitor='val_loss', factor = 0.1, patience=10)\n",
    "]\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(16, (3, 3), activation='relu', input_shape=IMAGE_SHAPE))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "\n",
    "#Add a classfier on top of the convnet\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(.5))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "#model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "#compile the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "3915/3915 [==============================] - 1208s 308ms/step - loss: 0.7085 - acc: 0.7828 - val_loss: 0.5839 - val_acc: 0.8362\n"
     ]
    }
   ],
   "source": [
    "history= model.fit_generator (\n",
    "           train_generator,\n",
    "           epochs = NUM_EPOCHS,\n",
    "           steps_per_epoch=LIMIT_BATCHES,\n",
    "           #callbacks = callbacks_list,\n",
    "           validation_data=validation_generator,\n",
    "           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualization of model performance\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values =  history_dict['val_loss']\n",
    "acc_values = history_dict['acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_values, 'bo', label = 'Training Loss')\n",
    "plt.plot(epochs,val_loss_values, 'b', label = 'Validation Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.xlabel='Epochs'\n",
    "plt.ylabel='Loss'\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
    "plt.plot(epochs,val_acc_values, 'b', label='Validation accuracy')\n",
    "plt.xlabel = 'Epochs'\n",
    "plt.ylabel = 'Loss'\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_filenames():\n",
    "    glob_filepath = os.path.join(ASL_ALPHABET_DATASET, '*_test.jpg')\n",
    "    filepaths_with_test_string = glob.glob(glob_filepath)\n",
    "    just_filenames = [ntpath.basename(f) for f in filepaths_with_test_string]\n",
    "    return just_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_from_filename(filename):\n",
    "    # example \"F_test.jpg\" will be just \"F\"\n",
    "    return re.split('_', filename)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_from_filename(filename):\n",
    "    filepath = os.path.join(ASL_ALPHABET_DATASET, filename)\n",
    "    img = load_img(filepath, target_size=(IMAGE_SHAPE[0], IMAGE_SHAPE[1]))\n",
    "    return img_to_array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_encoded(label):\n",
    "    encoding = np.zeros(NUM_CLASSES)\n",
    "    index = train_generator.class_indices[label]\n",
    "    encoding[index] = 1\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data():\n",
    "    test_filenames = get_test_filenames()\n",
    "    labels = [get_label_from_filename(filename) for filename in test_filenames]\n",
    "    label_encoded = [get_label_encoded(label) for label in labels]\n",
    "    images = [load_image_from_filename(filename) for filename in test_filenames]\n",
    "    return (images, label_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to create \"../data/models/asl_alphabet\" but it already exists\n",
      "28/28 [==============================] - 0s 6ms/step\n",
      "saving file ../data/models/asl_alphabet/2018-11-25 21:02:43___0.75.h5\n"
     ]
    }
   ],
   "source": [
    "def ensure_model_directory_exists():\n",
    "    try:\n",
    "        os.mkdir(SAVED_MODEL_DIR)\n",
    "    except:\n",
    "        print('Tried to create \"{}\" but it already exists'.format(SAVED_MODEL_DIR))\n",
    "\n",
    "def save_model():\n",
    "    ensure_model_directory_exists()\n",
    "    \n",
    "    evaluate_model_with_test_data = model.evaluate([x_test], np.array(y_test))\n",
    "\n",
    "    accuracy_of_model = evaluate_model_with_test_data[-1]\n",
    "    timestamp_raw = str(datetime.datetime.now())\n",
    "    timestamp_without_milliseconds = timestamp_raw.split('.')[0]\n",
    "    \n",
    "    model_filename = '{}___{}.h5'.format(timestamp_without_milliseconds, accuracy_of_model)\n",
    "    model_filepath = os.path.join(SAVED_MODEL_DIR, model_filename)\n",
    "    \n",
    "    print('saving file {}'.format(model_filepath))\n",
    "\n",
    "    model.save(model_filepath)\n",
    "    return accuracy_of_model\n",
    "mae = save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text sent\n"
     ]
    }
   ],
   "source": [
    "# Send text message when complete (if the environment variable \"TEXT_PHONENUMBER\" is set)\n",
    "if 'TEXT_PHONENUMBER' in os.environ:\n",
    "    text_phonenumber = os.environ['TEXT_PHONENUMBER']\n",
    "    client = boto3.client('sns')\n",
    "    client.publish(PhoneNumber=text_phonenumber, Message='Model Finished with mae {}'.format(mae))\n",
    "    print('text sent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
